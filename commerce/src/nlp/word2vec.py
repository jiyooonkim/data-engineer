# -*- coding: utf-8 -*-
"""
# title : ìƒí’ˆëª…ì— ì í•©í•œ inner keyword í›„ë³´ ì¶”ì¶œ ìœ„í•´
# desc :
    - inner keyword : ê²€ìƒ‰ì— ì‚¬ìš©í•˜ëŠ” í•„í„°ë§ í‚¤ì›Œë“œ
# document:
    - skip-gram :
        - https://wikidocs.net/69141
        - https://everyday-deeplearning.tistory.com/entry/Python%EC%9C%BC%EB%A1%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%ED%95%98%EA%B8%B0%EC%9E%90%EC%97%B0%EC%96%B4-2%EC%9E%84%EB%B2%A0%EB%94%A9-Word2Vec
# pro :
     - ì†ì„±, ë¶ˆìš©ì–´(stopword) ì œê±°
     - ìƒí’ˆëª…ì—ì„œ inner keyword ì œê±° í›„ ë‚¨ì€ ê²ƒì´ Stopword ê°€ ë  ê°€ëŠ¥ì„±ì€ ??
 # insight :
    - ì—°ê´€í‚¤ì›Œë“œ : window ë‚´ì— ê°€ì¥ ë§ì´ ë“±ì¥í•œ í‚¤ì›Œë“œ ì„¸íŠ¸
        ex) íˆí”„ì»¤ë²„->í™ì»¤ë²„, ë‚šì‹œë³µ, ë‚šì‹œì˜·, ë‚šì‹œìš©í’ˆ ..., íë§ì‰´ë“œ-> í°íŠ¸ë¦¬, ì•¡ì •ë³´í˜¸í•„ë¦„, ì‹œê³„ë³´í˜¸í•„ë¦„, ê³¨í”„ -> ê³¨í”„ì²´, ì›¨ì–´, ë‚¨ì„±, ê°€ë°©, ì—¬ì„±
    - ì˜¤íƒ€êµì • : ì´ˆ,ì¤‘,ì¢…ì„±ì˜ ì•,ë’¤ë¡œ ì˜¬ í™•ë¥  êµ¬í•  ìˆ˜ ìˆì„ë“¯ : ì •íƒ€ì‚¬ì „ ë§Œë“¤ì–´ ë³´ê¸°!!
"""

'''
word2vec(word embedding)
- word to vector
- ì´ë¡  : ìì£¼ ê°™ì´ ë“±ì¥í• ìˆ˜ë¡ ë‘ ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒ
- skip-gram : ì¤‘ê°„ë‹¨ì–´ë¡œ ë§¥ë½(ì£¼ë³€) ì˜ˆì¸¡
- cbow : ë§¥ë½(ì£¼ë³€) ì¤‘ê°„ë‹¨ì–´ ì˜ˆì¸¡

-
format
ìƒí’ˆëª…      innerkwd

'''
############################
import os
from pyspark.sql import SparkSession
from pyspark.ml.feature import Word2Vec
import pyspark.sql.functions as F
import pyspark.sql.types as T
import pyspark.sql.window as window

os.chdir('../../../')


# @F.udf(returnType=T.ArrayType(T.ArrayType(T.StringType())))
# def get_embadding_layer(col_lst=[]):
#     '''
#         :param  i : ì…ë ¥1(embedding layer1)   j : ì…ë ¥2(embedding layer2)
#         window size = 1
#     '''
#     lst = []
#     for i in col_lst:
#         for j in col_lst:
#             if i != j:
#                 lst.append([i, j])
#     return lst

@F.udf(returnType=T.ArrayType(T.ArrayType(T.StringType())))
def sliding_window(tokens=[], window_size=3):
    """
        :param tokens: í† í°ë¦¬ìŠ¤íŠ¸ : ['í™€ë”', 'ìŠ¤íƒ ë“œ', 'ì‚¼ê°ëŒ€', 'í”Œë ‰ì‹œë¸”', 'í—¤ë“œ', 'í•«ìŠˆ', 'ì¡°ì¸íŠ¸', 'ë“€ì–¼']
        :param window_size: ìŠ¬ë¼ì´ìŠ¤ ì‚¬ì´ì¦ˆ
        :return: ['í™€ë”', 'ìŠ¤íƒ ë“œ', 'ì‚¼ê°ëŒ€']
    """
    skip_gram = list()
    for idx in range(len(tokens)):
        if idx - window_size < 0:
            skip_gram.append(tokens[0: idx + window_size + 1])
        else:
            skip_gram.append(tokens[idx - window_size: idx + window_size + 1])
    return skip_gram


@F.udf(returnType=T.ArrayType(T.ArrayType(T.StringType())))
def get_skip_gram(tokens=[], center="", window_size=3):
    """
        :param tokens: ['í”Œë ‰ì‹œë¸”', 'í—¤ë“œ', 'í•«ìŠˆ', 'ì¡°ì¸íŠ¸', 'ë“€ì–¼']
        :param window_size: 2
        :return: [['í•«ìŠˆ', 'í”Œë ‰ì‹œë¸”'], ['í•«ìŠˆ', 'í—¤ë“œ'], ['í•«ìŠˆ', 'ì¡°ì¸íŠ¸'], ['í•«ìŠˆ', 'ë“€ì–¼']]
    """

    def skip_grams(center, tkns):
        grams = list()
        for i in tkns:
            if center != i:
                grams.append([center, i])
        return grams

    res = list()
    for idx in range(len(tokens)):
        if center == tokens[idx]:
            if idx - window_size < 0:
                res.extend(skip_grams(tokens[idx], tokens[0: idx + window_size + 1]))
            else:
                res.extend(skip_grams(tokens[idx], tokens[idx - window_size: idx + window_size + 1]))
    return res


@F.udf(returnType=T.StringType())
def get_txt_type(col):
    val = ''
    if col.isalpha():  # eng+kor, kor
        val = 'kor'
    if col.encode().isalpha():  # only eng
        val = 'eng'
    if col.isdigit():  # ìˆ«ìë§Œ
        val = 'num'
    # isalnum() # ì˜ì–´/í•œê¸€ + ìˆ«ì
    return val


if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName('Word 2 vector Job') \
        .master('local[*]') \
        .config('spark.sql.execution.arrow.pyspark.enabled', True) \
        .config('spark.sql.session.timeZone', 'UTC') \
        .config('spark.driver.memory', '32g') \
        .config('spark.driver.cores', '8') \
        .config('spark.executor.memory', '16g') \
        .config('spark.submit.deployMode', 'client') \
        .config("spark.driver.bindAddress", "127.0.0.1") \
        .config("spark.network.timeout", 100000) \
        .config('spark.ui.showConsoleProgress', True) \
        .config('spark.sql.repl.eagerEval.enabled', True) \
        .getOrCreate()

    ''' skip-gram '''
    prod_df = spark.read.parquet('data/parquet/prod2/').select(
        F.regexp_replace(
            F.lower(F.trim(F.col('prod_nm'))), '\s+', ' '
        ).alias('prod_nm')
    ).withColumn(
        "prod_nm_tkns",
        F.split(F.regexp_replace(F.lower(F.col('prod_nm')), ' ', ','), ",")
    ).withColumn(
        "target_word",
        F.explode(F.col('prod_nm_tkns'))
    ).distinct().alias("prod_df")

    skipgram = (prod_df
                .withColumn("skip_gram", get_skip_gram(F.col('prod_nm_tkns'), F.col('target_word'), F.lit(2)))
                .repartition(350)
                .alias("skipgram"))

    embd_lble = skipgram.select(
        F.col('prod_nm'),
        F.explode(F.col('skip_gram')).alias('embad_layer'),
        F.col('embad_layer')[0].alias('layer1'),
        F.col('embad_layer')[1].alias('layer2')
    ).where(
        (F.length(F.col('layer1')) > 1) &
        (F.length(F.col('layer2')) > 1)
    ).alias('embd_lble')
    # embd_lble.orderBy(F.col('prod_nm')).show(100, False)

    # negative sampling : ì˜ˆì¸¡ê°’ êµ¬í•˜ê¸°, ë ˆì´ë¸”ì´ 1ì¸ê²ƒë§Œ ê°€ëŠ¥, 0ì€ ë¶ˆê°€(ì¼ë¶€ë§Œ ìƒ˜í”Œë§í•˜ëŠ”ê²ƒì¸ë° ì¼ë¶€ì˜ ê¸°ì¤€ ì• ë§¤í•´ì„œ..)
    kor_eng_lble_frq = embd_lble \
        .withColumn('txt_type', get_txt_type(F.col('layer1'))) \
        .where(F.col('txt_type') == 'kor') \
        .groupBy(F.col('layer1'), F.col('layer2')) \
        .agg(F.count(F.col('layer2')).alias('cnt')) \
        .where(F.col('cnt') > 10) \
        .alias("kor_eng_lble_frq")

    # layer2 -> ìƒìœ„ 4ê°œê¹Œì§€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ìœ¼ë¡œ
    get_candidate = kor_eng_lble_frq \
        .withColumn(
            'prdt_val',
            F.round(F.log((F.col('cnt') / kor_eng_lble_frq.count())), 4)
        ).withColumn(
            'lyr2_rnk',
            F.rank().over(window.Window.partitionBy(F.col('layer1')).orderBy(F.col('prdt_val')))
        ).where(F.col('lyr2_rnk') < 6).alias('get_candidate')

    stop_word = spark.read.parquet("data/parquet/stop_word_1").alias('stop_word')
    except_stopword = get_candidate.join(
        stop_word,
        F.col('get_candidate.layer1') == F.col('stop_word.prod_tkn'),
        'leftanti'
    )

    (except_stopword.coalesce(20).write.format("parquet").mode("overwrite")
     .save("data/parquet/word2vec/skip_gram/cnadidate"))

    ''' Word2Vec library test (Only ë²¡í„°í™” ì‹œì¼œì£¼ëŠ” ê¸°ëŠ¥) '''
    # word2Vec = Word2Vec(vectorSize=20, seed=3, inputCol="layer1", outputCol="model")
    # word2Vec.setMaxIter(10)
    # model = word2Vec.fit(except_stopword)
    # model.getVectors().show(100, False)

    '''
        ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ skip-gram(SGNS)
        ì¤‘ì‹¬ë‹¨ì–´ & ì£¼ë³€ë‹¨ì–´ ë§¤í•‘ í•´ì„œ ì „ì²´ í™•ë¥  êµ¬í•´ë³´ê¸°
        ì„¸íŠ¸ë¡œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì˜ë¼ë³´ë©´...?
        ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì˜ ëª©ì ì€ íƒ€ê²Ÿ ë‹¨ì–´ì™€ ì—°ê´€ì„±ì´ ì—†ì„ ê²ƒì´ë¼ê³  ì¶”ì •ë˜ëŠ” ë‹¨ì–´ë¥¼ ë½‘ëŠ” ê²ƒ
        negative samplingì„ í•  ë•ŒëŠ” ë” ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ ì—°ê´€ì„±ì´ ë‚®ì„ ê²ƒ
        
       ë¶„ì : ğ‘“(ğ‘¤ğ‘–) ëŠ” í•´ë‹¹ ë‹¨ì–´ê°€ ë§ë­‰ì¹˜ì— ë“±ì¥í•œ ë¹„ìœ¨(í•´ë‹¹ ë‹¨ì–´ ë¹ˆë„/ì „ì²´ ë‹¨ì–´ìˆ˜)
       ë¶„ëª¨ : ì‚¬ì‹¤ ì¤‘ë³µì„ í—ˆìš©í•œ ì „ì²´ ë‹¨ì–´ì˜ ìˆ˜
    '''
    # todo : ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ì§„í–‰ í•˜ê¸°
    
    skipgram.show(100, False)
    # # /usr/local/Cellar/hadoop/3.3.4/libexec/bin/hdfs

    # attr = spark.read.parquet("data/parquet/measures_attribution/")   # ì†ì„± df
    # attr.show(100, False)

    exit(0)
