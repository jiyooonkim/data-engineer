### [Hard(Tech) Skill] 
+ Q: 경험한 분산 처리 경험에 대해 설명해 주세요. 어떤 프로젝트에서 어떻게 활용하셨나요?     
Apache Tool(spark, Hive, Hadoop, Sqoop, Airflow), Shell    
두번째 회사에 재직할때 온프레미스 환경이었습니다. 
ETL, ETL을 통한 DW와 DM 구축을하였습니다.(다양한 분석과 서비스를 위한 댐을 구축했다 보시면 될 것 같습니다.)
데이터 랭킹 서비스 출시에 pyspark 사용하여 데이터 분석(트렌드키워드 랭킹 키워드 추출, 상품정보 추출)을 하였습니다.
이외 에도 pyspark을 사용하여 클렌징한 데이터를 생산해내는 일을 하였습니다.
위 과정들을 Airflow로 전부 배치와 준배치로 스케줄링 개발을 하였습니다.

ElasticSearch


Apache Tool(spark, Hive, Hadoop, Sqoop, Airflow), Shell  
세번째 회사에서는 하이브리드 환경(클라우드가 조금 더 많은)이었습니다.
OLAP 와 OLTP를 위한 ETL과 ELT 데이터 파이프라인 구축을 하였습니다.
데이터 파이프라인


AWS(Athena, S3, Glue)
Databricks, EleasticSearch

+ Q: 데이터 모델링 및 데이터 아키텍처 설계 경험에 대해 구체적으로 설명해 주세요.
+ 
+ Q: AWS와 같은 클라우드 기반 컴퓨팅 플랫폼을 사용한 경험이 있나요? 어떤 문제를 해결했는지 설명해 주세요.
+ 
+ Q: SQL 및 NoSQL 데이터베이스를 사용한 경험이 있나요? 각각의 장단점에 대해 설명해 주세요.
+ 
+ Q: Java, Scala, Python 중 어떤 언어를 주로 사용하시나요? 해당 언어를 사용한 프로젝트 경험을 공유해 주세요.



### [Soft(Communitate) Skill]
+ Q: 다양한 팀과 협업하여 데이터 요구사항을 파악하고 KPI를 정의한 경험이 있나요? 어떻게 접근하셨나요?
+ Q: 데이터 품질 문제를 발견하고 해결한 경험이 있나요? 구체적인 사례를 설명해 주세요.
+ Q: 동료 또는 사용자들의 문제를 해결하기 위한 열정을 어떻게 보여주셨나요? 예시를 들어 설명해 주세요.
+ Q: 데이터 기반의 의사결정을 지원하기 위해 어떤 노력을 하셨나요?
+ Q: 핀테크 기술 조직과의 협업 경험이 있나요? 그 과정에서 어떤 도전과제를 극복하셨나요?

